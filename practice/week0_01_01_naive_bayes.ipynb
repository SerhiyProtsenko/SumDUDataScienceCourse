{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 01: Naive Bayes from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will write Naive Bayes classifier supporting different feature probabilities.\n",
    "\n",
    "_Authors: [Radoslav Neychev](https://github.com/neychev), [Vladislav Goncharenko](https://github.com/v-goncharenko)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:08:51.904850Z",
     "start_time": "2020-02-11T07:08:50.413258Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First to load dataset we're going to use [`sklearn`](https://scikit-learn.org/stable/) package which we will extensively use during the whole course.\n",
    "\n",
    "`sklearn` implement most of classical and frequently used algorithms in Machine Learning. Also it provides [User Guide](https://scikit-learn.org/stable/user_guide.html) describing principles of every bunch of algorithms implemented.\n",
    "\n",
    "As an entry point to main `sklearn`'s concepts we recommend [getting started tutorial](https://scikit-learn.org/stable/getting_started.html) (check it out yourself). [Further tutorials](https://scikit-learn.org/stable/tutorial/index.html) can also be handy to develop your skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First functionality we use is cosy loading of [common datasets](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets). All we need to do is just one function call.\n",
    "\n",
    "Object generated by [`load_iris`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) is described as:\n",
    "\n",
    "> Dictionary-like object, the interesting attributes are:\n",
    ">\n",
    "> ‘data’, the data to learn,\n",
    ">\n",
    ">‘target’, the classification labels,\n",
    ">\n",
    ">‘target_names’, the meaning of the labels,\n",
    ">\n",
    ">‘feature_names’, the meaning of the features,\n",
    ">\n",
    ">‘DESCR’, the full description of the dataset,\n",
    ">\n",
    ">‘filename’, the physical location of iris csv dataset (added in version 0.20)\n",
    "\n",
    "Let's see what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:08:51.918857Z",
     "start_time": "2020-02-11T07:08:51.910566Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_iris()\n",
    "\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you aren't familiar with Iris dataset - take a minute to read description above =) (as always [more info about it in Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set))\n",
    "\n",
    "__TL;DR__ 150 objects equally distributed over 3 classes each described with 4 continuous features\n",
    "\n",
    "Just pretty table to look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:08:51.940271Z",
     "start_time": "2020-02-11T07:08:51.921326Z"
    }
   },
   "outputs": [],
   "source": [
    "# for now you don't need to understand what happens in this code - just look at the table\n",
    "ext_target = dataset.target[:, None]\n",
    "# ext_target = np.expand_dims(dataset.target, axis=-1)\n",
    "df = pd.DataFrame(\n",
    "    np.concatenate((dataset.data, ext_target, dataset.target_names[ext_target]), axis=1),\n",
    "    columns=dataset.feature_names + ['target label', 'target name'],\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now give distinct names to the data we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:08:52.604007Z",
     "start_time": "2020-02-11T07:08:52.599704Z"
    }
   },
   "outputs": [],
   "source": [
    "features = dataset.data\n",
    "target = dataset.target\n",
    "\n",
    "features.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Please, remember!!!__\n",
    "\n",
    "Anywhere in our course we have an agreement to shape design matrix (named `features` in code above) as \n",
    "\n",
    "`(#number_of_items, #number_of_features)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has 4 dimensions however humans are more common to 3 or even 2 dimensional data, so let's plot first 3 features colored with labels values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:09:01.519543Z",
     "start_time": "2020-02-11T07:09:01.508622Z"
    }
   },
   "outputs": [],
   "source": [
    "# projection \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(features[:, 0], features[:, 1], features[:, 3], c=target, marker='o')\n",
    "ax.set_xlabel(dataset.feature_names[0])\n",
    "ax.set_ylabel(dataset.feature_names[1])\n",
    "ax.set_zlabel(dataset.feature_names[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:09:02.191167Z",
     "start_time": "2020-02-11T07:09:01.870454Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3d plot\n",
    "import plotly.express as px\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig = px.scatter_3d(df, x='sepal length (cm)', y='sepal width (cm)', z='petal width (cm)',\n",
    "                    color='target name')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then have a look on feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:09:03.615686Z",
     "start_time": "2020-02-11T07:09:02.922717Z"
    }
   },
   "outputs": [],
   "source": [
    "# remember this way to make subplots! It could be useful for you later in your work\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for i, axis in enumerate(axes.flat):\n",
    "    axis.hist(features[:, i])\n",
    "    axis.set_xlabel(dataset.feature_names[i])\n",
    "    axis.set_ylabel('number of objects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that every plot above have own scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we aiming to implement Naive Bayes algorithm first we need some prior distribution defined.\n",
    "\n",
    "The most common distribution is (of course) Gaussian and it's params are mean and standard deviation. Let's implement class taking list of feature values, estimating distribution params and able to give probability density of any given feature value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote the normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ PDF:\n",
    "$$\n",
    "f(x|\\mu, \\sigma^2) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp(-\\frac{(x - \\mu)^2}{2\\sigma^2})\n",
    "$$\n",
    "Let's implement the `GaussianDistribution` class. (Of course in practice one could always use something like `scipy.stats.norm`).\n",
    "\n",
    "Please note, that making computations with log probabilities is more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:09:05.944368Z",
     "start_time": "2020-02-11T07:09:05.938558Z"
    }
   },
   "outputs": [],
   "source": [
    "class GaussianDistribution:\n",
    "    def __init__(self, feature):\n",
    "        '''\n",
    "        Args:\n",
    "            feature: column of design matrix, represents all available values\n",
    "                of feature to model.\n",
    "                axis=0 stays for samples.\n",
    "        '''\n",
    "        self.mean = feature.mean(axis=0)\n",
    "        self.std = feature.std(axis=0)\n",
    "\n",
    "    def logpdf(self, value):\n",
    "        '''Logarithm of probability density at value'''\n",
    "        return  # <YOUR CODE HERE>\n",
    "    \n",
    "    def pdf(self, value):\n",
    "        return  # <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "_test = scipy.stats.norm(loc=features[:, :2].mean(axis=0), scale=features[:, :2].std(axis=0))\n",
    "assert np.allclose(\n",
    "    GaussianDistribution(features[:, :2]).logpdf(features[:5, :2]),\n",
    "    _test.logpdf(features[:5, :2])\n",
    ")\n",
    "print('Seems fine!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T00:09:57.874598Z",
     "start_time": "2020-02-11T00:09:57.740553Z"
    }
   },
   "source": [
    "### [Probabilistic model](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Probabilistic_model)\n",
    "\n",
    "Let's focus on the classification problem now. For the case of $K$ classes label $y_i \\in \\{C_1, \\ldots, C_k\\}$. Iris classification problem has 3 classes, so $K=3$. Bayes' Theorem takes the following form:\n",
    "\n",
    "$$\n",
    "P(y_i = C_k|\\mathbf{x}_i) = \\frac{P(\\mathbf{x}_i|y_i = C_k) P(y_i = C_k)}{P(\\mathbf{x}_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, we prefer working with log probabilities here as well. So the equation above will take the following form:\n",
    "$$\n",
    "\\log P(y_i = C_k|\\mathbf{x}_i) = \\log P(\\mathbf{x}_i|y_i = C_k) + \\log P(y_i = C_k) - \\log P(\\mathbf{x}_i)\n",
    "$$\n",
    "\n",
    "As one could mention, to find the class label with the highest probability we even do not need the last term $P(\\mathbf{x}_i)$. However, we need it to get the correct estimation of the probability $P(y_i = C_k|\\mathbf{x}_i)$. \n",
    "\n",
    "The $P(\\mathbf{x}_i)$ term can be computed using the following property:\n",
    "$$\n",
    "P(\\mathbf{x}_i) = \\sum_{k=1}^K P(y_i = C_k)  P(\\mathbf{x}_i|y_i=C_k).\n",
    "$$\n",
    "It can be computed from $\\log P(\\mathbf{x}_i|y_i=C_k)$ values using `logsumexp` function located in `scipy.special`.\n",
    "\n",
    "Now let's implement the Naive Bayes classifier itself. The class below is inherited from `sklearn` base classes and provides all the main methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:30:45.357053Z",
     "start_time": "2020-02-11T07:30:45.346489Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "class NaiveBayes(BaseEstimator, ClassifierMixin):\n",
    "    '''\n",
    "    Please note, using `X` and `y` for design matrix and labels in general is not a good choice,\n",
    "    better stick to more informative naming conventions.\n",
    "    However, to make the code consistent with sklearn implementation, we use `X` and `y` variables here.\n",
    "    # '''\n",
    "    def fit(self, X, y, sample_weight=None, distributions=None):\n",
    "        '''\n",
    "        sample_weight \n",
    "            The argument is ignored. For comatibility only.\n",
    "        '''\n",
    "        assert sample_weight is None\n",
    "        self.unique_labels = np.unique(y)\n",
    "        \n",
    "        # If distributions of features are not specified, they a treated Gaussian\n",
    "        if distributions is None:\n",
    "            distributions = [GaussianDistribution] * X.shape[1]\n",
    "        else:\n",
    "            # Check whether distributions are passed for all features\n",
    "            assert len(distributions) == X.shape[1]        \n",
    "\n",
    "        # Here we find distribution parameters for every feature in every class subset\n",
    "        # so P(x^i|y=C_k) will be estimated only using information from i-th feature of C_k class values\n",
    "        self.conditional_feature_distributions = {} # label: [distibution for feature 1, ...]\n",
    "        for label in self.unique_labels:\n",
    "            feature_distribution = []\n",
    "            for column_index in range(X.shape[1]):\n",
    "                # `column_index` feature values for objects from `label` class\n",
    "                feature_column = X[y == label, column_index]\n",
    "                fitted_distr = distributions[column_index](feature_column)\n",
    "                feature_distribution.append(fitted_distr)\n",
    "            self.conditional_feature_distributions[label] = feature_distribution\n",
    "\n",
    "        # Prior label distributions (unconditional probability of each class)\n",
    "        self.prior_label_distibution = {\n",
    "            # <YOUR CODE HERE>\n",
    "        }\n",
    "\n",
    "    \n",
    "    def predict_log_proba(self, X):\n",
    "        # Matrix of shape (n_objects : n_classes)\n",
    "        class_log_probas = np.zeros((X.shape[0], len(self.unique_labels)), dtype=float)\n",
    "        \n",
    "        # Here we compute the class log probabilities for each class sequentially b\n",
    "        for label_idx, label in enumerate(self.unique_labels):\n",
    "            for idx in range(X.shape[1]):\n",
    "                # All loglikelihood for every feature w.r.t. fixed label\n",
    "\n",
    "                class_log_probas[:, label_idx] += # <YOUR CODE HERE>\n",
    "\n",
    "            # Add log proba of label prior\n",
    "            # <YOUR CODE HERE>\n",
    "\n",
    "        for idx in range(X.shape[1]):\n",
    "        # If you want to get probabilities, you need to substract the log proba for every feature\n",
    "            # <YOUR CODE HERE>\n",
    "        return class_log_probas\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return np.exp(self.predict_log_proba(X))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        log_probas = self.predict_log_proba(X)\n",
    "        # we need to cast labels to their original form (they may start from number other than 0)\n",
    "        return np.array([self.unique_labels[idx] for idx in log_probas.argmax(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(features, target)\n",
    "print('log probas:\\n{}'.format(nb.predict_log_proba(features[:2])))\n",
    "print('predicted labels:\\n{}'.format(nb.predict(features[:2])))\n",
    "print('\\nIt`s alive! More tests coming.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check our Naive Bayes classifier on the unseed data. To do so we will use `train_test_split` from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.25, random_state=24)\n",
    "\n",
    "print(features_train.shape, features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:30:45.713112Z",
     "start_time": "2020-02-11T07:30:45.709195Z"
    }
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(features_train, target_train, distributions=[GaussianDistribution]*4)\n",
    "nb_test_log_proba = nb.predict_log_proba(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Naive Bayes classifier accuracy on the train set: {}'.format(nb.score(features_train, target_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Naive Bayes classifier accuracy on the test set: {}'.format(nb.score(features_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's comapre the Naive Bayes classifier with the `sklearn` implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:30:47.513267Z",
     "start_time": "2020-02-11T07:30:47.498843Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "sklearn_nb = naive_bayes.GaussianNB()\n",
    "sklearn_nb.fit(features_train, target_train)\n",
    "sklearn_nb_test_log_proba = sklearn_nb.predict_log_proba(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sklearn implementation accuracy on the train set: {}'.format(sklearn_nb.score(features_train, target_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sklearn implementation accuracy on the test set: {}'.format(sklearn_nb.score(features_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's even check the predictions. If you used Gaussian distribution and done everything correctly, the log probabilities should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:30:47.853360Z",
     "start_time": "2020-02-11T07:30:47.841855Z"
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(nb_test_log_proba, sklearn_nb_test_log_proba), 'log probabilities do not match'\n",
    "print('Seems alright!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing to kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(features_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(features_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seem like Naive Bayes classifier performance is comparable to the kNN, while Naive Bayes does not need to store all the train data (while kNN need)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced distribution for NaiveBayes\n",
    "\n",
    "Let's take a look at violin plots for every feature in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.columns[:4]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 15), sharey=True)\n",
    "plt.violinplot(features, showmedians=True)\n",
    "ax.set_xticks(np.arange(1, len(labels) + 1))\n",
    "ax.set_xticklabels(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we do love Gaussian distribution it is still unimodal while our features are substantially multimodal (see histograms above). So we have to implement more robust distribution estimator - Kernel Density Estimator (KDE).\n",
    "\n",
    "Idea for this method is simple: we assign some probability density to a region around actual observation. (We will return to density estimation methods to describe them carefully later in this course).\n",
    "\n",
    "Fortunately `sklearn` have KDE implemented for us already. All it needs is vector of feature values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get probability estimations using KDE one can easily access the `sklearn.neighbors` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T00:27:57.223204Z",
     "start_time": "2020-02-11T00:27:54.891Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "kde = KernelDensity(bandwidth=0.28, kernel='gaussian')\n",
    "feature_col = features[target==2, 2]\n",
    "kde.fit(feature_col.reshape((-1, 1)))\n",
    "linspace = np.linspace(feature_col.min(), feature_col.max(), 1000)\n",
    "plt.plot(linspace, np.exp(kde.score_samples(linspace.reshape((-1, 1)))))\n",
    "plt.grid()\n",
    "plt.xlabel('feature value')\n",
    "plt.ylabel('probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it compatible with the Naive Bayes classifier we have implemented above, we need to create class with the same methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T00:27:57.237856Z",
     "start_time": "2020-02-11T00:27:54.896Z"
    }
   },
   "outputs": [],
   "source": [
    "class GaussianKDE:\n",
    "    def __init__(self, feature):\n",
    "        self.kde = KernelDensity(bandwidth=1.)\n",
    "        self.kde.fit(feature.reshape((-1, 1)))\n",
    "\n",
    "    def logpdf(self, value):\n",
    "        return self.kde.score_samples(value.reshape((-1, 1)))\n",
    "\n",
    "    def pdf(self, value):\n",
    "        return np.exp(self.log_proba(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_kde = NaiveBayes()\n",
    "nb_kde.fit(features, target, distributions=[GaussianKDE]*4)\n",
    "print('log probas:\\n{}'.format(nb_kde.predict_log_proba(features[:2])))\n",
    "print('predicted labels:\\n{}'.format(nb_kde.predict(features[:2])))\n",
    "print('\\nIt`s alive!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KDE Naive Bayes classifier accuracy on the train set: {}'.format(nb_kde.score(features_train, target_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('KDE Naive Bayes classifier accuracy on the test set: {}'.format(nb_kde.score(features_test, target_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the accuracy of the classifier has decreased. What is going on?\n",
    "\n",
    "_Hint: try varying the `bandwidth` parameter of the `KernelDensity` constructor in `GaussianKDE` class (around 0.3)._\n",
    "\n",
    "Let's take a closer look on the features distributions. Here comes the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for ax_idx, feature_idx in enumerate([2, 3]):\n",
    "    for label in range(3):\n",
    "        ax = axes[ax_idx, label]\n",
    "        feature_col = features[target==label, feature_idx]\n",
    "        ax.hist(feature_col, bins=7)\n",
    "        ax.grid()\n",
    "        ax.set_title('class: {}, feature: {}'.format(\n",
    "            dataset.target_names[label],\n",
    "            dataset.feature_names[feature_idx]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see, than the distributions within every class are unimodal. That's how KDE is approximating the PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "kde = KernelDensity(bandwidth=1., kernel='gaussian')\n",
    "\n",
    "for ax_idx, feature_idx in enumerate([2, 3]):\n",
    "    for label in range(3):\n",
    "        ax = axes[ax_idx, label]\n",
    "        feature_col = features[target==label, feature_idx]\n",
    "        kde.fit(feature_col.reshape((-1, 1)))\n",
    "        linspace = np.linspace(\n",
    "            0.8*feature_col.min(),\n",
    "            1.2*feature_col.max(),\n",
    "            1000\n",
    "        )\n",
    "        ax.plot(linspace, np.exp(kde.score_samples(linspace.reshape((-1, 1)))))\n",
    "        ax.grid()\n",
    "        ax.set_title('class: {}, feature: {}'.format(\n",
    "            dataset.target_names[label],\n",
    "            dataset.feature_names[feature_idx]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One could mention, that every feature need different `bandwidth` parameter.\n",
    "\n",
    "And that's how Gaussian distribution fits to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for ax_idx, feature_idx in enumerate([2, 3]):\n",
    "    for label in range(3):\n",
    "        ax = axes[ax_idx, label]\n",
    "        feature_col = features[target==label, feature_idx]\n",
    "        gaussian_distr = GaussianDistribution(feature_col)\n",
    "        linspace = np.linspace(\n",
    "            feature_col.min(),\n",
    "            feature_col.max(),\n",
    "            1000\n",
    "        )\n",
    "        ax.plot(linspace, gaussian_distr.pdf(linspace.reshape((-1, 1))))\n",
    "        ax.grid()\n",
    "        ax.set_title('class: {}, feature: {}'.format(\n",
    "            dataset.target_names[label],\n",
    "            dataset.feature_names[feature_idx]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks a bit better. Moreover, hypothesis of the normal distribution over the features seems more promising (the features are petal length and width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "for ax_idx, feature_idx in enumerate([2, 3]):\n",
    "    for label in range(3):\n",
    "        ax = axes[ax_idx, label]\n",
    "        feature_col = features[target==label, feature_idx]\n",
    "        ax.hist(feature_col, bins=7, density=True, alpha=0.4)\n",
    "                \n",
    "        bandwidth = abs((feature_col.min() - feature_col.max()) / 5)\n",
    "        kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')\n",
    "        kde.fit(feature_col.reshape((-1, 1)))\n",
    "        linspace = np.linspace(\n",
    "            feature_col.min(),\n",
    "            feature_col.max(),\n",
    "            1000\n",
    "        )\n",
    "        ax.plot(linspace, np.exp(kde.score_samples(linspace.reshape((-1, 1)))), c='m', label='KDE')\n",
    "        \n",
    "        gaussian_distr = GaussianDistribution(feature_col)\n",
    "        linspace = np.linspace(\n",
    "            feature_col.min(),\n",
    "            feature_col.max(),\n",
    "            1000\n",
    "        )\n",
    "        ax.plot(linspace, gaussian_distr.pdf(linspace.reshape((-1, 1))), c='g', label='Gaussian')        \n",
    "        \n",
    "        ax.grid()\n",
    "        ax.set_title('class: {}, feature: {}'.format(\n",
    "            dataset.target_names[label],\n",
    "            dataset.feature_names[feature_idx]\n",
    "        ))\n",
    "\n",
    "labels_handles = {\n",
    "  label: handle for ax in fig.axes for handle, label in zip(*ax.get_legend_handles_labels())\n",
    "}\n",
    "fig.legend(\n",
    "  labels_handles.values(),\n",
    "  labels_handles.keys(),\n",
    "  loc=\"lower center\",\n",
    "  bbox_to_anchor=(0.5, 0),\n",
    "  bbox_transform=plt.gcf().transFigure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the __conclusion__: always check the distribution and the assumptions you make. They should be appropriate for the data you work with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
